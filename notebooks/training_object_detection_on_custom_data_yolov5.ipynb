{"cells":[{"cell_type":"markdown","metadata":{"id":"7mGmQbAO5pQb"},"source":["#Install Dependencies\n","\n","_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2390,"status":"ok","timestamp":1724341694678,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"Ie5uLDH4uzAp","outputId":"77d906a0-1569-4150-d3d5-dbc6ad37177e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 16858, done.\u001b[K\n","remote: Counting objects: 100% (53/53), done.\u001b[K\n","remote: Compressing objects: 100% (47/47), done.\u001b[K\n","remote: Total 16858 (delta 19), reused 35 (delta 6), pack-reused 16805 (from 1)\u001b[K\n","Receiving objects: 100% (16858/16858), 15.54 MiB | 21.16 MiB/s, done.\n","Resolving deltas: 100% (11548/11548), done.\n","/content/yolov5\n"]}],"source":["# clone YOLOv5 repository\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68328,"status":"ok","timestamp":1724341763004,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"wbvMlHd_QwMG","outputId":"c3bd4e8e-85a0-4818-e124-0363bb931a1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.1/869.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hSetup complete. Using torch 2.3.1+cu121 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15102MB, multi_processor_count=40)\n"]}],"source":["# install dependencies as necessary\n","!pip install -qr requirements.txt  # install dependencies (ignore errors)\n","import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","from utils.downloads import attempt_download  # to download models/datasets\n","\n","# clear_output()\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"]},{"cell_type":"markdown","metadata":{"id":"vDeebwqS9JbZ"},"source":["## Step 6: Download a Dataset\n","\n","Add your Roboflow API key below to download the default money counting dataset. Alternatively, use the code provided by the Roboflow dashboard in the above step to load a custom dataset."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4612,"status":"ok","timestamp":1724341767611,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"Sq4_kFaDIKg0","outputId":"064864d9-1024-4a12-a2f6-3ce77722d915"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/79.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["#to install the dataset\n","!pip install -q roboflow"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14478,"status":"ok","timestamp":1724341782081,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"Knxi2ncxWffW","outputId":"c68e07a3-6e6b-47cd-d7a4-acdec6180174"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.41)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.7.4)\n","Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n","Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n","loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"name":"stderr","output_type":"stream","text":["Downloading Dataset Version Zip in gunDetection-1 to yolov5pytorch:: 100%|██████████| 223883/223883 [00:08<00:00, 27704.18it/s]"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\n","Extracting Dataset Version Zip to gunDetection-1 in yolov5pytorch:: 100%|██████████| 2508/2508 [00:01<00:00, 2122.16it/s]\n"]}],"source":["!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"ai3wsRPz2BvVHVo522Rn\")\n","project = rf.workspace(\"learning-evfd1\").project(\"gundetection-0d8i9-6kdnb\")\n","version = project.version(1)\n","dataset = version.download(\"yolov5\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1724341782082,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"2_CNTEk-zSrO","outputId":"ddcef779-4baf-43d3-ec33-9237271591b0"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/yolov5'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["%pwd"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1724341782082,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"PK74WgnozeCY","outputId":"bbf5236c-5a26-490d-80f3-06bf37dbf3d8"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/yolov5/gunDetection-1'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset.location"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1724341782082,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"Ug_PhK1oqwQA","outputId":"6f4bd94d-0c65-4692-bc98-42a2ddb95b00"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/yolov5\n"]}],"source":["%cd /content/yolov5"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1724341782082,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"36-7YBd2M8kb","outputId":"b958834a-7c9f-4662-987e-1c4bf72a9a49"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/yolov5/gunDetection-1'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["dataset.location"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1724341782083,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"ZZ3DmmGQztJj","outputId":"ed18914e-fa89-487d-81da-d4f4d785d553"},"outputs":[{"name":"stdout","output_type":"stream","text":["names:\n","- Covered Face\n","- Hand Gun\n","- Person\n","- knife\n","nc: 4\n","roboflow:\n","  license: CC BY 4.0\n","  project: gundetection-0d8i9-6kdnb\n","  url: https://universe.roboflow.com/learning-evfd1/gundetection-0d8i9-6kdnb/dataset/1\n","  version: 1\n","  workspace: learning-evfd1\n","test: ../test/images\n","train: gunDetection-1/train/images\n","val: gunDetection-1/valid/images\n"]}],"source":["# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n","%cat {dataset.location}/data.yaml"]},{"cell_type":"markdown","metadata":{"id":"UwJx-2NHsYxT"},"source":["# Define Model Configuration and Architecture\n","\n","We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n","\n","You do not need to edit these cells, but you may."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1724341782083,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"dOPn9wjOAwwK"},"outputs":[],"source":["# define number of classes based on YAML\n","import yaml\n","with open(dataset.location + \"/data.yaml\", 'r') as stream:\n","    num_classes = str(yaml.safe_load(stream)['nc'])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1724341782083,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"PtsVZIVoz0f5","outputId":"73e5bdfd-22bf-475a-c28c-68a9651c2a64"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'4'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["num_classes"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1724341782083,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"1Rvt5wilnDyX","outputId":"d281fee6-73a4-498d-8858-5634c9c34a10"},"outputs":[{"name":"stdout","output_type":"stream","text":["# Ultralytics YOLOv5 🚀, AGPL-3.0 license\n","\n","# Parameters\n","nc: 80 # number of classes\n","depth_multiple: 0.33 # model depth multiple\n","width_multiple: 0.50 # layer channel multiple\n","anchors:\n","  - [10, 13, 16, 30, 33, 23] # P3/8\n","  - [30, 61, 62, 45, 59, 119] # P4/16\n","  - [116, 90, 156, 198, 373, 326] # P5/32\n","\n","# YOLOv5 v6.0 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [\n","    [-1, 1, Conv, [64, 6, 2, 2]], # 0-P1/2\n","    [-1, 1, Conv, [128, 3, 2]], # 1-P2/4\n","    [-1, 3, C3, [128]],\n","    [-1, 1, Conv, [256, 3, 2]], # 3-P3/8\n","    [-1, 6, C3, [256]],\n","    [-1, 1, Conv, [512, 3, 2]], # 5-P4/16\n","    [-1, 9, C3, [512]],\n","    [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32\n","    [-1, 3, C3, [1024]],\n","    [-1, 1, SPPF, [1024, 5]], # 9\n","  ]\n","\n","# YOLOv5 v6.0 head\n","head: [\n","    [-1, 1, Conv, [512, 1, 1]],\n","    [-1, 1, nn.Upsample, [None, 2, \"nearest\"]],\n","    [[-1, 6], 1, Concat, [1]], # cat backbone P4\n","    [-1, 3, C3, [512, False]], # 13\n","\n","    [-1, 1, Conv, [256, 1, 1]],\n","    [-1, 1, nn.Upsample, [None, 2, \"nearest\"]],\n","    [[-1, 4], 1, Concat, [1]], # cat backbone P3\n","    [-1, 3, C3, [256, False]], # 17 (P3/8-small)\n","\n","    [-1, 1, Conv, [256, 3, 2]],\n","    [[-1, 14], 1, Concat, [1]], # cat head P4\n","    [-1, 3, C3, [512, False]], # 20 (P4/16-medium)\n","\n","    [-1, 1, Conv, [512, 3, 2]],\n","    [[-1, 10], 1, Concat, [1]], # cat head P5\n","    [-1, 3, C3, [1024, False]], # 23 (P5/32-large)\n","\n","    [[17, 20, 23], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5)\n","  ]\n"]}],"source":["#this is the model configuration we will use for our tutorial\n","%cat /content/yolov5/models/yolov5s.yaml"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1724341782084,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"t14hhyqdmw6O"},"outputs":[],"source":["#customize iPython writefile so we can write variables\n","from IPython.core.magic import register_line_cell_magic\n","\n","@register_line_cell_magic\n","def writetemplate(line, cell):\n","    with open(line, 'w') as f:\n","        f.write(cell.format(**globals()))"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1724341782086,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"uDxebz13RdRA"},"outputs":[],"source":["%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n","\n","# parameters\n","nc: {num_classes}  # number of classes\n","depth_multiple: 0.33  # model depth multiple\n","width_multiple: 0.50  # layer channel multiple\n","\n","# anchors\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, BottleneckCSP, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 9, BottleneckCSP, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, BottleneckCSP, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 1, SPP, [1024, [5, 9, 13]]],\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n","  ]\n","\n","# YOLOv5 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]"]},{"cell_type":"markdown","metadata":{"id":"VUOiNLtMP5aG"},"source":["# Train Custom YOLOv5 Detector\n","\n","### Next, we'll fire off training!\n","\n","\n","Here, we are able to pass a number of arguments:\n","- **img:** define input image size\n","- **batch:** determine batch size\n","- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n","- **data:** set the path to our yaml file\n","- **cfg:** specify our model configuration\n","- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n","- **name:** result names\n","- **nosave:** only save the final checkpoint\n","- **cache:** cache images for faster training"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":536719,"status":"ok","timestamp":1724342318781,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"1NcFxRcFdJ_O","outputId":"3af100ff-916b-4572-dea3-34d387b78167"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/yolov5\n","2024-08-22 15:49:50.154490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-22 15:49:50.461453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-22 15:49:50.540197: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=./models/custom_yolov5s.yaml, data=/content/yolov5/gunDetection-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5s_results, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v7.0-356-g2070b303 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 41.5MB/s]\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 223MB/s]\n","\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  3    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n","  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","custom_YOLOv5s summary: 233 layers, 7263185 parameters, 7263185 gradients\n","\n","Transferred 223/369 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 59 weight(decay=0.0), 70 weight(decay=0.0005), 62 bias\n","INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.13). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/gunDetection-1/train/labels... 956 images, 39 backgrounds, 0 corrupt: 100% 956/956 [00:00<00:00, 1493.26it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/gunDetection-1/train/labels.cache\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram): 100% 956/956 [00:08<00:00, 111.14it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/gunDetection-1/valid/labels... 228 images, 9 backgrounds, 0 corrupt: 100% 228/228 [00:00<00:00, 599.83it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/gunDetection-1/valid/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 228/228 [00:03<00:00, 62.53it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.10 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to runs/train/yolov5s_results/labels.jpg... \n","Image sizes 416 train, 416 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/yolov5s_results\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/49      1.68G     0.1029    0.04179     0.0412         89        416: 100% 60/60 [00:17<00:00,  3.43it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.73it/s]\n","                   all        228        779     0.0018      0.148    0.00715    0.00156\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/49      1.98G      0.094    0.04804    0.03486         82        416: 100% 60/60 [00:11<00:00,  5.11it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  2.86it/s]\n","                   all        228        779      0.778     0.0439     0.0133    0.00344\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/49      1.98G    0.08828    0.04816    0.02923         83        416: 100% 60/60 [00:09<00:00,  6.06it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  2.78it/s]\n","                   all        228        779      0.739      0.116     0.0204    0.00535\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/49      1.98G    0.08388     0.0477    0.02544         97        416: 100% 60/60 [00:11<00:00,  5.14it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  3.66it/s]\n","                   all        228        779      0.467      0.166     0.0656     0.0178\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/49      1.98G    0.07969    0.04415    0.02157         71        416: 100% 60/60 [00:13<00:00,  4.58it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.18it/s]\n","                   all        228        779      0.394      0.171       0.12     0.0358\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/49      1.98G    0.07514    0.04189    0.01644         64        416: 100% 60/60 [00:13<00:00,  4.32it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.07it/s]\n","                   all        228        779      0.452      0.259      0.169     0.0574\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/49      1.98G    0.07214    0.03915    0.01306         57        416: 100% 60/60 [00:13<00:00,  4.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.03it/s]\n","                   all        228        779      0.533      0.371      0.265     0.0986\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/49      1.98G    0.06807    0.03805    0.01093         62        416: 100% 60/60 [00:12<00:00,  4.94it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  3.03it/s]\n","                   all        228        779      0.563      0.407      0.293      0.108\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/49      1.98G    0.06507    0.03742   0.009354         57        416: 100% 60/60 [00:10<00:00,  5.78it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  2.98it/s]\n","                   all        228        779      0.607       0.39      0.366      0.113\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/49      1.98G    0.06288     0.0365   0.009003         90        416: 100% 60/60 [00:11<00:00,  5.17it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.16it/s]\n","                   all        228        779      0.669      0.414      0.407      0.165\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/49      1.98G    0.06138    0.03549    0.00838         77        416: 100% 60/60 [00:13<00:00,  4.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.26it/s]\n","                   all        228        779      0.724      0.449      0.479      0.204\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      11/49      1.98G    0.05861    0.03432   0.007647         47        416: 100% 60/60 [00:13<00:00,  4.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.17it/s]\n","                   all        228        779      0.705      0.481      0.509      0.232\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      12/49      1.98G    0.05666    0.03366   0.007252         68        416: 100% 60/60 [00:13<00:00,  4.52it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.19it/s]\n","                   all        228        779      0.707        0.5       0.49      0.215\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      13/49      1.98G    0.05612    0.03315   0.006641         80        416: 100% 60/60 [00:12<00:00,  4.77it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  3.21it/s]\n","                   all        228        779      0.733      0.497      0.529      0.246\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      14/49      1.98G    0.05443    0.03352   0.006345         94        416: 100% 60/60 [00:10<00:00,  5.74it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  2.94it/s]\n","                   all        228        779      0.655      0.516      0.589      0.278\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      15/49      1.98G      0.054    0.03284   0.006435         80        416: 100% 60/60 [00:10<00:00,  5.66it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  3.72it/s]\n","                   all        228        779      0.585      0.643      0.602      0.291\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      16/49      1.98G    0.05243    0.03305   0.005786         86        416: 100% 60/60 [00:12<00:00,  4.67it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.10it/s]\n","                   all        228        779      0.668      0.592      0.627      0.311\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      17/49      1.98G    0.05182    0.03225   0.005751         65        416: 100% 60/60 [00:13<00:00,  4.32it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.19it/s]\n","                   all        228        779      0.568      0.625      0.592      0.295\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      18/49      1.98G    0.05015    0.03152   0.005215         83        416: 100% 60/60 [00:13<00:00,  4.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.28it/s]\n","                   all        228        779      0.638       0.65      0.641      0.329\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      19/49      1.98G     0.0496    0.03095   0.005117         85        416: 100% 60/60 [00:13<00:00,  4.46it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  3.65it/s]\n","                   all        228        779      0.813      0.639      0.666      0.338\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      20/49      1.98G     0.0492     0.0306   0.005068         78        416: 100% 60/60 [00:11<00:00,  5.24it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  2.77it/s]\n","                   all        228        779      0.688      0.632      0.645      0.333\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      21/49      1.98G      0.048    0.03053   0.005016         60        416: 100% 60/60 [00:10<00:00,  5.94it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  2.89it/s]\n","                   all        228        779      0.706      0.617      0.649      0.349\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      22/49      1.98G    0.04769    0.02993   0.004661         87        416: 100% 60/60 [00:11<00:00,  5.34it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.19it/s]\n","                   all        228        779      0.703      0.657      0.679      0.364\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      23/49      1.98G    0.04706    0.03084   0.004528         80        416: 100% 60/60 [00:13<00:00,  4.54it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.39it/s]\n","                   all        228        779      0.657      0.688      0.687      0.363\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      24/49      1.98G    0.04573    0.03044   0.004461         74        416: 100% 60/60 [00:13<00:00,  4.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.31it/s]\n","                   all        228        779      0.611       0.66      0.646      0.343\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      25/49      1.98G    0.04459    0.02845   0.004285         68        416: 100% 60/60 [00:13<00:00,  4.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  3.57it/s]\n","                   all        228        779      0.752      0.639      0.686      0.366\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      26/49      1.98G    0.04402     0.0297   0.004224         91        416: 100% 60/60 [00:10<00:00,  5.58it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  2.94it/s]\n","                   all        228        779      0.749      0.664      0.699      0.377\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      27/49      1.98G    0.04343    0.02785   0.003784         77        416: 100% 60/60 [00:10<00:00,  5.92it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:02<00:00,  3.55it/s]\n","                   all        228        779      0.747      0.653      0.702      0.388\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      28/49      1.98G    0.04341    0.02868   0.003951         72        416: 100% 60/60 [00:11<00:00,  5.14it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.31it/s]\n","                   all        228        779      0.676      0.716      0.698      0.391\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      29/49      1.98G    0.04247    0.02863   0.003977         81        416: 100% 60/60 [00:13<00:00,  4.49it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.33it/s]\n","                   all        228        779      0.757      0.685      0.702      0.403\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      30/49      1.98G    0.04183    0.02757   0.003626         92        416: 100% 60/60 [00:12<00:00,  4.78it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.31it/s]\n","                   all        228        779      0.741      0.701      0.704      0.401\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      31/49      1.98G    0.04183    0.02769   0.003918         71        416: 100% 60/60 [00:13<00:00,  4.48it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:01<00:00,  4.05it/s]\n","                   all        228        779      0.685      0.725      0.686      0.391\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      32/49      1.98G    0.04056    0.02943    0.00388        100        416:  20% 12/60 [00:02<00:08,  5.67it/s]\n","Traceback (most recent call last):\n","  File \"/content/yolov5/train.py\", line 986, in <module>\n","    main(opt)\n","  File \"/content/yolov5/train.py\", line 688, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"/content/yolov5/train.py\", line 421, in train\n","    scaler.scale(loss).backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n","    _engine_run_backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n","    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","CPU times: user 6.32 s, sys: 728 ms, total: 7.05 s\n","Wall time: 8min 56s\n"]}],"source":["# train yolov5s on custom data for 200 epochs\n","# time its performance\n","%%time\n","%cd /content/yolov5/\n","!python train.py --img 416 --batch 16 --epochs 50 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights 'yolov5s.pt' --name yolov5s_results  --cache"]},{"cell_type":"markdown","metadata":{"id":"kJVs_4zEeVbF"},"source":["# Evaluate Custom YOLOv5 Detector Performance"]},{"cell_type":"markdown","metadata":{"id":"7KN5ghjE6ZWh"},"source":["You can view the training graphs associated with a training job in the `/content/yolov5/runs/train/yolov5s_results/results.png` folder.\n","\n","Training losses and performance metrics are also saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov5s_results`.\n","\n","Note from Glenn: Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"elapsed":2272,"status":"error","timestamp":1724342321047,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"C60XAsyv6OPe","outputId":"d0601a27-3997-4db7-f42b-f6da0e8d4c0f"},"outputs":[],"source":["from utils.plots import plot_results  # plot results.txt as results.png\n","Image(filename='/content/yolov5/runs/train/yolov5s_results/results.png', width=1000)  # view results.png"]},{"cell_type":"markdown","metadata":{"id":"DLI1JmHU7B0l"},"source":["### Curious? Visualize Our Training Data with Labels\n","\n","After training starts, view `train*.jpg` images to see training images, labels and augmentation effects.\n","\n","Note a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Glenn Jocher and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1724342321047,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"PF9MLHDb7tB6"},"outputs":[],"source":["Image(filename='/content/yolov5/runs/train/yolov5s_results/val_batch0_labels.jpg', width=900)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1724342321047,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"W40tI99_7BcH"},"outputs":[],"source":["# print out an augmented training example\n","print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n","Image(filename='/content/yolov5/runs/train/yolov5s_results/train_batch0.jpg', width=900)"]},{"cell_type":"markdown","metadata":{"id":"N3qM6T0W53gh"},"source":["# Run Inference With Trained Weights\n","\n","Next, we can run inference with a pretrained checkpoint on all images in the `test/images` folder to understand how our model performs on our test set."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1724342321047,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"yIEwt5YLeQ7P"},"outputs":[],"source":["# trained weights are saved by default in our weights folder\n","%ls runs/"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1724342321047,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"4SyOWS80qR32"},"outputs":[],"source":["%ls runs/train/yolov5s_results/weights"]},{"cell_type":"markdown","metadata":{"id":"0h3D6Ms2Z_ZU"},"source":["In the snippet below, replace `Cash-Counter-10` with the name of the folder in which your dataset is stored."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8557,"status":"ok","timestamp":1724342509906,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"9nmZZnWOgJ2S","outputId":"d6098cc9-10f4-4d48-8e1f-f42dd40c1b5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/yolov5\n","\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/yolov5s_results/weights/best.pt'], source=/content/yolov5/gunDetection-1/test/images/videoForGunDetection-Made-with-Clipchamp_mp4-0223_jpg.rf.d7b5682f169700337f889a3caf92a349.jpg, data=data/coco128.yaml, imgsz=[416, 416], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 🚀 v7.0-356-g2070b303 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","custom_YOLOv5s summary: 182 layers, 7254609 parameters, 0 gradients\n","WARNING ⚠️ NMS time limit 0.550s exceeded\n","image 1/1 /content/yolov5/gunDetection-1/test/images/videoForGunDetection-Made-with-Clipchamp_mp4-0223_jpg.rf.d7b5682f169700337f889a3caf92a349.jpg: 256x416 1 Covered Face, 1 Hand Gun, 2 Persons, 87.1ms\n","Speed: 0.5ms pre-process, 87.1ms inference, 1059.7ms NMS per image at shape (1, 3, 416, 416)\n","Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n"]}],"source":["%cd /content/yolov5/\n","!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source /content/yolov5/gunDetection-1/test/images/videoForGunDetection-Made-with-Clipchamp_mp4-0223_jpg.rf.d7b5682f169700337f889a3caf92a349.jpg"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1724342321048,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"odKEqYtTgbRc"},"outputs":[],"source":["import glob\n","from IPython.display import Image, display\n","\n","for imageName in glob.glob('/content/yolov5/runs/detect/exp2/*.jpg')[:10]: #assuming JPG\n","    display(Image(filename=imageName))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1724342321048,"user":{"displayName":"Budige Avinash","userId":"05291384720509196288"},"user_tz":-330},"id":"Ao0HwGoVqNrY"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
